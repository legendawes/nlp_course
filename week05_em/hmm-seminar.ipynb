{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    \n",
    "    for n, sequence in enumerate(sequences):\n",
    "        prev_char = None\n",
    "        for n_char, char in enumerate(sequence):\n",
    "            if n_char == 0:\n",
    "                pi_counts[char] += 1\n",
    "            \n",
    "            if prev_char is not None:\n",
    "                A_counts[prev_char, char] += 1\n",
    "            \n",
    "            prev_char = char\n",
    "#         assert False, \"Collect counts for pi and A and return parameter estimates.\"\n",
    "#     return pi_counts, A_counts\n",
    "    pi = pi_counts / pi_counts.sum()\n",
    "    A = A_counts / A_counts.sum(axis=0)\n",
    "    assert(np.allclose(A.argmax(axis=0), A_counts.argmax(axis=0)))\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping)\n",
    "        self.num_outputs = len(observations_to_char_mapping)\n",
    "        self.states_to_char_mapping = states_to_char_mapping\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping\n",
    "       \n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)\n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states)\n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs)\n",
    "        self.B /= np.sum(self.B, 1, keepdims=True) \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        if self.fixed_pi:\n",
    "            self.pi = parameters['pi']\n",
    "        self.fixed_A = 'A' in parameters\n",
    "        if self.fixed_A:\n",
    "            self.A = parameters['A']\n",
    "        self.fixed_B = 'B' in parameters\n",
    "        if self.fixed_B:\n",
    "            self.B = parameters['B']\n",
    "    \n",
    "        previous_llh = None\n",
    "        iter = 0\n",
    "        while True and iter < max_iters:\n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences)\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts)\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            print('iteration %d; log likelihood %.4f' % (iter, log_likelihood))\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iter += 1\n",
    "\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            # Run Forward-Backward dynamic program\n",
    "            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sequence)\n",
    "  \n",
    "            # Accumulate statistics.\n",
    "            pi_counts += gamma[:, 0]\n",
    "            A_counts += xi\n",
    "            for t, x in enumerate(sequence):\n",
    "                B_counts[:, x] += gamma[:, t]\n",
    "            \n",
    "            total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence)\n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        gamma = (alpha * beta) / np.sum(alpha * beta, 0)\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A)\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    this_xi[i, j] += alpha[i, t] * self.A[i, j] * beta[j, t+1] * self.B[j, sequence[t+1]]        \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        for t_plus1, x_t_plus1 in enumerate(sequence):\n",
    "            for i in range(self.num_states):\n",
    "                if t_plus1 == 0:\n",
    "                    x_0 = x_t_plus1\n",
    "                    \n",
    "                    alpha[i, 0] = self.pi[i] * self.B[i, x_0]\n",
    "                else:\n",
    "                    new = 0.0\n",
    "                    for j in range(self.num_states):\n",
    "                        new += alpha[j, t_plus1 - 1] * self.A[j, i]\n",
    "                        \n",
    "                    alpha[i][t_plus1] = self.B[i, x_t_plus1] * new\n",
    "                    \n",
    "#         assert False, \"Implement forward recursion\"\n",
    "        return alpha \n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        \n",
    "        for ind, x_t_plus1 in enumerate(sequence[::-1]):\n",
    "            t_plus1 = len(sequence) - ind\n",
    "            \n",
    "            for i in range(self.num_states):\n",
    "                t = t_plus1 - 1\n",
    "                if t_plus1 == len(sequence):\n",
    "                    beta[i][t] = 1.0\n",
    "                else:\n",
    "                    new = 0.0\n",
    "                    for j in range(self.num_states):\n",
    "                        new += beta[j, t_plus1] * self.A[i, j] * self.B[j, x_t_plus1]\n",
    "                        \n",
    "                    beta[i][t] = new\n",
    "#             print(beta[0, :])\n",
    "        \n",
    "#         assert False, \"Implement backwards recursion to compute betas.\"\n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = [self.observations_to_char_mapping[x] for x in sequence]                \n",
    "            map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sequence)]\n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(states):       th     le    e  z\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       a       an     a r   e   j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       aee  e    o t  e  we t e h   we t  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       te h oe     ee e       eeoe t w\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       teo n ee wn e  e   ae t  h   we t  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       te     tee oe  e x\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       an   s he  e we t n   e  e   r q\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       a       e  q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       an     e   i we t n  eex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       a        th t    e  e   e  h ee  e h    r  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 0; log likelihood -909.3370\n",
      "(states):       ah     me  t e  j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       a       an     a w   e t j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       aee  e    o n th  we t enh   we n  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ae h oh t  the       t eeo  n w\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       eeo n  e wn a  e   ae n  h   we n  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ae     tae oe  e q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       an   s he  e we t n t e ta     q\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       a          q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       an         w we t n t ex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       a        nh t  t e  en t   h ae  enh    r  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 1; log likelihood -759.8677\n",
      "(states):       ahe t  ae  t e  j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       a      thn     a oat e t j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       wee  e t  o n  hatoe t enh   we n  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ar h oh t  the     t t eeo  n w\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       eeo n  e wi a  a   ae nt h  t e n  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ar     tae oo  e q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       an   s he  e we t n   e ta     q\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       a        t q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       an     o   w  e t n   ex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       a        nh t  t e  en  h  h ae  enh    w  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 2; log likelihood -754.5830\n",
      "(states):       ahe t eat  tie  j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       an  oe thi     he at e t j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       wae  e t  o i  oat e t ent s  e i  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ar hiohit    e     t t eeo  n w\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       ero i  e oi aa a   ha id te t e i  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ar     tae ooa e q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       an     he  e  eet i   e the  o q\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       an  oe  ot q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       an     o   wt eet i   ex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       an  oe   it t    e  en  o  teae  ent s  w  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 3; log likelihood -751.9492\n",
      "(states):       ate t eat   ir  j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       ae  oe  his e  he a  eat j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       war  eat  oeid  at ee  eit s  eei  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ar t ot   a  e     t   een  i w\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       ero id e oinaa an  ta id te   eei  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ar  e   he ooa e q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       tis e  te  e  ee  id  eathe  oeq\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       ae  oe   t q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       tis e  o  e t ee  id  ex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       ae  oe   it t a  e  eid    teae  eit s  w  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 4; log likelihood -750.0294\n",
      "(states):       ath   eat   iv  v\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       ae   e  tis e  te an bat j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       war  ba   ouin  an be  bit a  bei  az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       avethot n a  e     t   ben  inw\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       ten in e oinaa an  tanin th   bei  az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ave eld he ooa h q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       tis e   h  b  be  in  bathe   uq\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       ae   e     q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       tis e     e   be  in  ex\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       ae   e  eithn a  b  bin    thaen bit an w  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 5; log likelihood -748.3398\n",
      "(states):       athen utt h iv  v\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       th   u  tiu e  th an ban j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       tarn ban  puin  an ben binhan beis az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ave ho hn an en nh  hn ben  inw\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       ten in enoieta hinhtanin then beis az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ave mll hhen a h q\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       tiu e   he be be  in  bante   us\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       th   u   e q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       tiu e   n h n be  in  ba\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       th   u  einhn an be bin  n tharn binhan w  az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 6; log likelihood -746.6309\n",
      "(states):       a hen utt heibe v\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       th   u etiu r ethomn ben j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       tarn ben  pulln an bun tlnd n buls az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ate henhn  n en nh  he ter hinw\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       ter in enoi ta thnhthlin  hen buls az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ate hlldther a heq\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       tiu r  nhe be bun il  benth   us\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       th   us  n q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       tiu r   n h n bun il  ba\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       th   u  ulnhn  n be bil  n  hare plnd n    az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 7; log likelihood -743.6972\n",
      "(states):       a hen ut e elty j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       ahe  uset u o ethomneten j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       tavneten erolle mn bundtlng n buls az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ate henge  n ee n    ester llno\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       thr ln heoihts the thlln  hen buls az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ate  aletter a bex\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       t u o ente te bun ile tenthe  us\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       ahe  use n q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       t u o e n hon bun ileepa\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       ahe  useulnge  n bedblle n  havedplng n  s az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 8; log likelihood -739.2161\n",
      "(states):       a ten ot e elty j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       ahe  osetlo oseahomeetee j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       thaneten erolle mndmond lng ndmols az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       ate  enged n he i y testhr llnf\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       thelln he ltas the ahiln  tendmols az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       ated aletter m bex\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       alo osenteste bon lleetenahe  os\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       ahe  ose i q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       alo ose is ondmon lleepa\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       ahe  oseolnged netedblle is tavedplng nd s az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 9; log likelihood -736.6361\n",
      "(states):       a teeso  iteloy j\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       ates orealororeat mev ee j\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       thane ee eeolle med ord lig ed ole az\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       a es enged e he it etes he  lef\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       thenle he ltar tnetahiles teed ole az\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       a ed anetter m mex\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       alororen es ed or llee ee tes os\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       ates ore i q\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       alorore ist id or llee m\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       ates oreoliged ee ed lle is thaed lig ed s az\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n",
      "iteration 10; log likelihood -738.0982\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cf6baaf70ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Estimate the parameters and decode the encrypted sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_with_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-d8274b908cc5>\u001b[0m in \u001b[0;36mestimate_with_em\u001b[0;34m(self, sequences, parameters, epsilon, max_iters)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration %d; log likelihood %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprevious_llh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mprevious_llh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_llh\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:10], parameters={'A' :A,'pi':pi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
