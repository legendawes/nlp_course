{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))\n",
    "    \n",
    "    for n, sequence in enumerate(sequences):\n",
    "        prev_char = None\n",
    "        for n_char, char in enumerate(sequence):\n",
    "            if n_char == 0:\n",
    "                pi_counts[char] += 1\n",
    "            \n",
    "            if prev_char is not None:\n",
    "                A_counts[prev_char, char] += 1\n",
    "            \n",
    "            prev_char = char\n",
    "            \n",
    "    pi = pi_counts / np.sum(pi_counts)\n",
    "    A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('q', 'u')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_char, next_char = np.unravel_index(A.argmax(), A.shape)\n",
    "int_to_char_mapping[prev_char], int_to_char_mapping[next_char]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha_{t+1}(i) = \\sum_{j} \\alpha_{t}(j)A_{j}(i)B_{i}(x_{t+1})$\n",
    "\n",
    "$\\beta_{t}(i) = \\sum_{j}\\beta_{t+1}A_{i}(j)B_{j}(x_{t+1})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}, seed=None,\n",
    "                 print_test_output=True, early_stopping_thresh=None):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping)\n",
    "        self.num_outputs = len(observations_to_char_mapping)\n",
    "        self.states_to_char_mapping = states_to_char_mapping\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping\n",
    "        self.print_test_output = print_test_output\n",
    "        self.early_stopping_thresh = early_stopping_thresh\n",
    "       \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)\n",
    "        self.pi /= np.sum(self.pi)\n",
    "        \n",
    "#         self.pi = np.ones((self.num_states)) / self.num_states\n",
    "        \n",
    "        self.A = np.random.rand(self.num_states, self.num_states)\n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        \n",
    "#         self.A = np.ones((self.num_states, self.num_states)) / self.num_states\n",
    "#       \n",
    "        \n",
    "        self.B = np.ones((self.num_states, self.num_outputs)) / self.num_outputs\n",
    "        self.B = self.B + np.random.rand(self.num_states, self.num_outputs) / 100\n",
    "        self.B = np.clip(self.B, 0, None)\n",
    "#         self.B = np.random.rand(self.num_states, self.num_outputs)\n",
    "        self.B /= np.sum(self.B, 1, keepdims=True)\n",
    "        \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        if self.fixed_pi:\n",
    "            print('pi is fixed')\n",
    "            self.pi = parameters['pi']\n",
    "        \n",
    "        self.fixed_A = 'A' in parameters\n",
    "        if self.fixed_A:\n",
    "            print('A is fixed')\n",
    "            self.A = parameters['A']\n",
    "            \n",
    "        self.fixed_B = 'B' in parameters\n",
    "        if self.fixed_B:\n",
    "            print('B is fixed')\n",
    "            self.B = parameters['B']\n",
    "    \n",
    "        previous_llh = None\n",
    "        iter = 0\n",
    "        while True and iter < max_iters:\n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences)\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts)\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "            if self.print_test_output:\n",
    "                self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            print('iteration %d; log likelihood %.4f' % (iter, log_likelihood))\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "                if self.early_stopping_thresh is not None:\n",
    "                    if abs(previous_llh - log_likelihood) < self.early_stopping_thresh:\n",
    "                        print(f'early stopping previous_llh {previous_llh}, log_likelihood {log_likelihood}')\n",
    "                        break\n",
    "                        \n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iter += 1\n",
    "\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            # Run Forward-Backward dynamic program\n",
    "            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sequence)\n",
    "  \n",
    "            # Accumulate statistics.а с\n",
    "            pi_counts += gamma[:, 0]\n",
    "            A_counts += xi\n",
    "            for t, x in enumerate(sequence):\n",
    "                B_counts[:, x] += gamma[:, t]\n",
    "            \n",
    "            total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = self.forward(sequence)\n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        gamma = (alpha * beta) / np.sum(alpha * beta, 0)\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = alpha[:, t].reshape(-1, 1) * self.A * beta[:, t+1] * self.B[:, sequence[t+1]]\n",
    "            \n",
    "#             this_xi = np.zeros_like(self.A)\n",
    "#             for i in range(self.num_states):\n",
    "#                 for j in range(self.num_states):\n",
    "#                     this_xi[i, j] += alpha[i, t] * self.A[i, j] * beta[j, t+1] * self.B[j, sequence[t+1]]\n",
    "#             assert(np.allclose(vector_this_xi, this_xi))\n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "                \n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1]))\n",
    "    \n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        \n",
    "        for t, xt in enumerate(sequence):\n",
    "            if t == 0:\n",
    "                prev_alpha = self.pi * self.B[:, xt]\n",
    "                alpha_initialized = True\n",
    "            else:\n",
    "                prev_alpha = self.A.T.dot(prev_alpha).flatten() * self.B[:, xt]\n",
    "            \n",
    "#             prev_alpha = prev_alpha / np.sum(prev_alpha)\n",
    "#             print(prev_alpha)\n",
    "            alpha[:, t] = prev_alpha.copy()\n",
    "        \n",
    "        assert(alpha_initialized)\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        num_states = len(self.pi)\n",
    "        beta = np.zeros((num_states, len(sequence)))\n",
    "    \n",
    "        for ind, xt in enumerate(sequence[::-1]):\n",
    "            t = len(sequence) - ind - 1\n",
    "            if ind == 0:\n",
    "                prev_beta = np.ones(num_states)\n",
    "                beta_initialized = True\n",
    "            else:\n",
    "                prev_beta = (prev_beta * self.B[:, xt_plus1]).dot(self.A.T)\n",
    "#                 for i in range(num_states):\n",
    "#                     for j in range(num_states):\n",
    "#                         beta[i, t] += beta[j, t+1] * self.A[j, i] * self.B[j, xt_plus1]\n",
    "#             prev_beta = prev_beta / np.sum(prev_beta)\n",
    "            beta[:, t] = prev_beta.copy()\n",
    "            xt_plus1 = xt\n",
    "    \n",
    "        assert(beta_initialized)\n",
    "        \n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        for i, sequence in enumerate(sequences):\n",
    "            \n",
    "            observations = [self.observations_to_char_mapping[x] for x in sequence]                \n",
    "            map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sequence)]\n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM_cycles(HMM):\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        \n",
    "        num_states = len(self.pi)\n",
    "        for t, xt in enumerate(sequence):\n",
    "            if t == 0:\n",
    "                alpha[:, 0] = self.pi * self.B[:, xt]\n",
    "                alpha_initialized = True\n",
    "            else:\n",
    "                for i in range(num_states):\n",
    "                    for j in range(num_states):\n",
    "                        alpha[i, t] += alpha[j, t - 1] * self.A[j, i] * self.B[i, xt]\n",
    "    \n",
    "        assert(alpha_initialized)\n",
    "        \n",
    "        return alpha\n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        num_states = len(self.pi)\n",
    "        beta = np.zeros((num_states, len(sequence)))\n",
    "        \n",
    "        for ind, xt in enumerate(sequence[::-1]):\n",
    "            t = len(sequence) - ind - 1\n",
    "            if ind == 0:\n",
    "                beta[:, t] = 1.0\n",
    "                beta_initialized = True\n",
    "            else:\n",
    "                for i in range(num_states):\n",
    "                    for j in range(num_states):\n",
    "                        beta[i, t] += beta[j, t + 1] * self.A[i, j] * self.B[j, xt_plus1]\n",
    "            xt_plus1 = xt\n",
    "\n",
    "        assert(beta_initialized)\n",
    "        \n",
    "        return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare numpy and cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "# state_to_char_mapping = int_to_char_mapping\n",
    "# observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# # Initialize a HMM with the correct state/output spaces.\n",
    "# hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "# hmm_cycles = HMM_cycles(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# hmm_cycles.pi = hmm.pi.copy()\n",
    "# hmm_cycles.A = hmm.A.copy()\n",
    "# hmm_cycles.B = hmm.B.copy()\n",
    "\n",
    "# for sequence in tqdm(encrypted_sequences[:100]):\n",
    "#     assert(np.allclose(hmm.forward(sequence), hmm_cycles.forward(sequence)))\n",
    "#     assert(np.allclose(hmm.backward(sequence), hmm_cycles.backward(sequence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is fixed\n",
      "A is fixed\n",
      "iteration 0; log likelihood -12069.1866\n",
      "iteration 1; log likelihood -10426.0091\n",
      "iteration 2; log likelihood -10414.7779\n",
      "iteration 3; log likelihood -10401.6116\n",
      "iteration 4; log likelihood -10383.4688\n",
      "iteration 5; log likelihood -10355.2655\n",
      "iteration 6; log likelihood -10308.6621\n",
      "iteration 7; log likelihood -10231.4049\n",
      "iteration 8; log likelihood -10108.4666\n",
      "iteration 9; log likelihood -9927.1098\n",
      "iteration 10; log likelihood -9696.0348\n",
      "iteration 11; log likelihood -9465.2005\n",
      "iteration 12; log likelihood -9280.0454\n",
      "iteration 13; log likelihood -9140.8159\n",
      "iteration 14; log likelihood -9028.4377\n",
      "iteration 15; log likelihood -8930.9627\n",
      "iteration 16; log likelihood -8847.6208\n",
      "iteration 17; log likelihood -8779.9658\n",
      "iteration 18; log likelihood -8727.4448\n",
      "iteration 19; log likelihood -8688.0165\n",
      "iteration 20; log likelihood -8658.8629\n",
      "iteration 21; log likelihood -8636.3325\n",
      "iteration 22; log likelihood -8618.3976\n",
      "iteration 23; log likelihood -8605.4603\n",
      "iteration 24; log likelihood -8596.0290\n",
      "iteration 25; log likelihood -8588.9340\n",
      "iteration 26; log likelihood -8583.4240\n",
      "iteration 27; log likelihood -8579.0204\n",
      "iteration 28; log likelihood -8575.4411\n",
      "iteration 29; log likelihood -8572.5027\n",
      "iteration 30; log likelihood -8570.0698\n",
      "iteration 31; log likelihood -8568.0436\n",
      "iteration 32; log likelihood -8566.3516\n",
      "iteration 33; log likelihood -8564.9335\n",
      "iteration 34; log likelihood -8563.7354\n",
      "iteration 35; log likelihood -8562.7114\n",
      "iteration 36; log likelihood -8561.8241\n",
      "early stopping previous_llh -8562.7113504462, log_likelihood -8561.824116730451\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, seed=None, print_test_output=False, early_stopping_thresh=1)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:100],\n",
    "                     parameters={'A':A, 'pi':pi},\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(states):       than such a roman\n",
      "(observations): noeixjtcoxexhwfei\n",
      "(states):       cassous brutus bait not me\n",
      "(observations): cejjgtjxkhtntjxkegnxiwnxfq\n",
      "(states):       ill not endure it you forget yourself\n",
      "(observations): gddxiwnxqi thqxgnxbwtxpwhvqnxbwthjqdp\n",
      "(states):       to hedge me in i am a soldier i\n",
      "(observations): nwxoq vqxfqxgixgxefxexjwd gqhxg\n",
      "(states):       older in prackice abler than yourself\n",
      "(observations): wd qhxgixyhecngcqxekdqhxnoeixbwthjqdp\n",
      "(states):       to mave conditions\n",
      "(observations): nwxfeaqxcwi gngwij\n",
      "(states):       brutus go to you are not cassous\n",
      "(observations): khtntjxvwxnwxbwtxehqxiwnxcejjgtj\n",
      "(states):       cassous i am\n",
      "(observations): cejjgtjxgxef\n",
      "(states):       brutus i say you are not\n",
      "(observations): khtntjxgxjebxbwtxehqxiwn\n",
      "(states):       cassous urge me no more i thall forget myself\n",
      "(observations): cejjgtjxthvqxfqxiwxfwhqxgxjoeddxpwhvqnxfbjqdp\n"
     ]
    }
   ],
   "source": [
    "hmm.output(encrypted_sequences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi is fixed\n",
      "A is fixed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:121: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:133: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:135: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -inf\n",
      "iteration 1; log likelihood nan\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-927638327506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Estimate the parameters and decode the encrypted sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m hmm.estimate_with_em(encrypted_sequences[:100],\n\u001b[0;32m---> 10\u001b[0;31m                      \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                     )\n",
      "\u001b[0;32m<ipython-input-7-182b224d878e>\u001b[0m in \u001b[0;36mestimate_with_em\u001b[0;34m(self, sequences, parameters, epsilon, max_iters)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration %d; log likelihood %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprevious_llh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mprevious_llh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprevious_llh\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping, seed=17, print_test_output=False, early_stopping_thresh=0.1)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:100],\n",
    "                     parameters={'A':A, 'pi':pi},\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence = [1, 2, 3, 27]\n",
    "# num_states = 27\n",
    "# num_output_states = 28\n",
    "\n",
    "# alpha = np.random.rand(num_states, len(sequence))\n",
    "# prev_alpha = np.random.rand(num_states, 1)\n",
    "# A = np.random.rand(num_states, num_states)\n",
    "# B = np.random.rand(num_states, num_output_states)\n",
    "\n",
    "def multiply_non_numpy_way(alpha, A, B, sequence):\n",
    "    num_states = len(alpha)\n",
    "    \n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    for t, xt in enumerate(sequence):\n",
    "        for i in range(num_states):\n",
    "            for j in range(num_states):\n",
    "                res[i, t] += alpha[j, t] * A[i, j] * B[i, xt]\n",
    "    return res\n",
    "\n",
    "def multiply_numpy_way(alpha, A, B, sequence):\n",
    "    num_states = len(alpha)\n",
    "    \n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    for t, xt in enumerate(sequence):\n",
    "        res[:, t] = A.dot(alpha[:, t]) * B[:, xt]\n",
    "    return res\n",
    "\n",
    "try:\n",
    "    np.allclose(multiply_numpy_way(alpha, A, B, sequence), multiply_non_numpy_way(alpha, A, B, sequence))\n",
    "except NameError:\n",
    "    print('skip check')\n",
    "    \n",
    "def multiply_non_numpy_way_prev(prev_alpha, A, B, sequence):\n",
    "    num_states = len(prev_alpha)\n",
    "    \n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    for t, xt in enumerate(sequence):\n",
    "        for i in range(num_states):\n",
    "            for j in range(num_states):\n",
    "                res[i, t] += prev_alpha[j, 0] * A[i, j] * B[i, xt]\n",
    "        prev_alpha = res[:, t].reshape(-1, 1)\n",
    "        \n",
    "    return res\n",
    "\n",
    "def multiply_numpy_way_prev(prev_alpha, A, B, sequence):\n",
    "    num_states = len(alpha)\n",
    "    \n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    for t, xt in enumerate(sequence):\n",
    "        res[:, t] = A.dot(prev_alpha).flatten() * B[:, xt]\n",
    "        prev_alpha = res[:, t].copy()\n",
    "        \n",
    "    return res\n",
    "\n",
    "try:\n",
    "    np.allclose(multiply_non_numpy_way_prev(prev_alpha, A, B, sequence), multiply_numpy_way_prev(prev_alpha, A, B, sequence))\n",
    "except NameError:\n",
    "    print('skip check')\n",
    "    \n",
    "\n",
    "def B_non_numpy_way(beta, A, B, sequence):\n",
    "    num_states = len(beta)\n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    \n",
    "    for t, xt in enumerate(sequence):\n",
    "        for i in range(num_states):\n",
    "            for j in range(num_states):\n",
    "                res[i, t] += beta[j, t] * A[j, i] * B[j, xt]\n",
    "    return res\n",
    "\n",
    "def B_numpy_way(beta, A, B, sequence):\n",
    "    num_states = len(beta)\n",
    "    res = np.zeros((num_states, len(sequence)))\n",
    "    \n",
    "    for t, xt in enumerate(sequence):\n",
    "#         res[i, t] += beta[j,t] * A[j,i] * B[j, xt]\n",
    "        res[:, t] = (beta[:,t] * B[:, xt]).dot(A)\n",
    "    return res\n",
    "\n",
    "\n",
    "try:\n",
    "    assert(np.allclose(B_numpy_way(beta, A, B, sequence), B_non_numpy_way(beta, A, B, sequence)))\n",
    "except NameError:\n",
    "    print('skip check')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
