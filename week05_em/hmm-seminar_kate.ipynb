{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hidden Markov models for cracking codes**\n",
    "\n",
    "In this exercise you have to make a partially built HMM work and use it to solve some simple substitution ciphers. Plaintext data is provided in 'plaintext' directory. Encrypted data is in 'encrypted'. Some of the texts were originally English some of them were Russian; the sequences are also of different lengths. \n",
    "\n",
    "This homework is worth **15 points** and is due by the next class (**24th Oct.**), please submit the results of the **TASK 5** (a list of files and names of the author/work) to Anytask in the following format: 'filename author' where 'filename' is a file from \"encrypted/\\*_encrypted.txt\" and 'author' is a file from \"plaintext/\\*.txt\" (not including 'english.txt', 'russian.txt' or 'all.txt') which best matches the decrypted text.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities for loading data from file and converting characters to integers and back.\n",
    "import numpy as np\n",
    "    \n",
    "def get_char_to_int_mapping(path):\n",
    "    # Load data from path and get mapping from characters to integers and back.\n",
    "    characters = set()\n",
    "    for line in open(path):\n",
    "        characters.update(set([c for c in line.strip()]))\n",
    "    char_to_int_mapping = dict([(char, i) for i, char in enumerate(sorted(list(characters)))])\n",
    "    int_to_char_mapping = [char for char, i in char_to_int_mapping.items()]\n",
    "    return char_to_int_mapping, int_to_char_mapping\n",
    "\n",
    "def load_sequences(path, char_to_int_mapping):\n",
    "    # Load data from path and map to integers using mapping.\n",
    "    return [[char_to_int_mapping[c] for c in line.strip()] for line in open(path)]\n",
    "\n",
    "def estimate_markov_model_from_sequences(sequences, num_states):\n",
    "    # Estimate a Markov model based on the sequences (integers) provided.\n",
    "    # pi[i] = Pr(s_0 = i)\n",
    "    pi_counts = np.zeros(num_states)\n",
    "    # A[i, j] = Pr(s_t = j | s_{t-1} = i)\n",
    "    A_counts = np.zeros((num_states, num_states))        \n",
    "    for n, sequence in enumerate(sequences):\n",
    "        if len(sequence) < 1:\n",
    "            continue\n",
    "        pi_counts[sequence[0]] += 1\n",
    "        for i in range(1, len(sequence)):\n",
    "            prev = sequence[i-1]\n",
    "            cur = sequence[i]\n",
    "            A_counts[prev, cur] += 1\n",
    "    pi = pi_counts / sum(pi_counts)\n",
    "    A = A_counts / sum(A_counts)\n",
    "\n",
    "#         assert False, \"Collect counts for pi and A and return parameter estimates.\"\n",
    "    \n",
    "    return pi, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1**: Make the following block run by completing the method 'estimate_markov_model_from_sequences' above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "plaintext = 'plaintext/english.txt'\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "# plaintext = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/1_encrypted.txt' # short sequences in english\n",
    "# ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a mostly implemented HMM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обоснование возможности нормализации:\n",
    "\n",
    "$$\n",
    "norm(\\alpha_{:t}) = \\sum_{j=0}^{n}\\alpha_{jt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overline{\\alpha_{it}} = \\frac{\\alpha_{it}}{norm(\\alpha_{:t})}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "norm(\\beta_{:t}) = \\sum_{j=0}^{n}\\beta_{jt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\overline{\\beta_{it}} = \\frac{\\beta_{it}}{norm(\\beta_{:t})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\gamma_{it} = \\frac{\\alpha_{it}  \\beta_{it}}{\\sum_{j=0}^n\\alpha_{jt} \\beta_{jt}} =\\frac{norm(\\alpha_{:t}) \\overline{\\alpha_{it}} norm(\\beta_{:t}) \\overline{\\beta_{it}}}{\\sum_{j=0}^nnorm(\\alpha_{:t}) \\overline{\\alpha_{jt}} norm(\\beta_{:t}) \\overline{\\beta_{jt}}} = \\\n",
    "\\frac{norm(\\alpha_{:t}) \\overline{\\alpha_{it}} norm(\\beta_{:t}) \\overline{\\beta_{it}}}{norm(\\alpha_{:t})norm(\\beta_{:t})\\sum_{j=0}^n \\overline{\\alpha_{jt}}  \\overline{\\beta_{jt}}} =\\\n",
    "\\frac{\\overline{\\alpha_{it}} \\overline{\\beta_{it}}}{\\sum_{j=0}^n \\overline{\\alpha_{jt}} \\overline{\\beta_{jt}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\xi_{ij} = \\sum_{t=1}^{T-1}\\frac{\\alpha_{it} a_{ij} \\beta_{jt+1} b_{jo_{t+1}}}{\\sum_i^n\\sum_j^n\\alpha_{it} a_{ij} \\beta_{jt+1} b_{jo_{t+1}}} = \\\n",
    "\\sum_{t=1}^{T-1}\\frac{norm(\\alpha_{:t}) \\overline{\\alpha_{it}} a_{ij} norm(\\beta_{:t+1}) \\overline{\\beta_{jt+1}} b_{jo_{t+1}}}{\\sum_i^n\\sum_j^nnorm(\\alpha_{:t}) \\overline{\\alpha_{it}} a_{ij} norm(\\beta_{:t+1}) \\overline{\\beta_{jt+1}} b_{jo_{t+1}}} = \\\n",
    "\\sum_{t=1}^{T-1}\\frac{\\overline{\\alpha_{it}} a_{ij} \\overline{\\beta_{jt+1}} b_{jo_{t+1}}}{\\sum_i^n\\sum_j^n\\overline{\\alpha_{it}} a_{ij} \\overline{\\beta_{jt+1}} b_{jo_{t+1}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "log\\sum_{i=0}^n \\alpha_{iT} = log\\sum_{i=0}^n norm(\\alpha_{:T}) \\overline{\\alpha_{iT}} = log[norm(\\alpha_{:T})\\sum_{i=0}^n  \\overline{\\alpha_{iT}}] = log(norm(\\alpha_{:T})) + log\\sum_{i=0}^n \\overline{\\alpha_{iT}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM():\n",
    "\n",
    "    def __init__(self, observations_to_char_mapping={}, states_to_char_mapping={}):\n",
    "        # Determine number of states and observation space. \n",
    "        self.num_states = len(states_to_char_mapping)\n",
    "        self.num_outputs = len(observations_to_char_mapping)\n",
    "        self.states_to_char_mapping = states_to_char_mapping\n",
    "        self.observations_to_char_mapping = observations_to_char_mapping\n",
    "       \n",
    "        np.random.seed(0)\n",
    "        # Random initialization\n",
    "        self.pi = np.random.rand(self.num_states)\n",
    "        self.pi /= np.sum(self.pi)\n",
    "        self.A = np.random.rand(self.num_states, self.num_states)\n",
    "        self.A /= np.sum(self.A, 1, keepdims=True)\n",
    "        self.B = np.random.rand(self.num_states, self.num_outputs)\n",
    "        self.B /= np.sum(self.B, 1, keepdims=True) \n",
    "        \n",
    "    def estimate_with_em(self, sequences, parameters={}, epsilon=0.001, max_iters=100):\n",
    "        # Estimates all parameters not provided in 'parameters' based on 'sequences'.\n",
    "        self.fixed_pi = 'pi' in parameters\n",
    "        if self.fixed_pi:\n",
    "            self.pi = parameters['pi']\n",
    "        self.fixed_A = 'A' in parameters\n",
    "        if self.fixed_A:\n",
    "            self.A = parameters['A']\n",
    "        self.fixed_B = 'B' in parameters\n",
    "        if self.fixed_B:\n",
    "            self.B = parameters['B']\n",
    "    \n",
    "        previous_llh = None\n",
    "        iter = 0\n",
    "        while True and iter < max_iters:\n",
    "            # Infer expected counts.\n",
    "            pi_counts, A_counts, B_counts, log_likelihood = self.e_step(sequences)\n",
    "\n",
    "            # Update parameters based on counts.\n",
    "            self.m_step(pi_counts, A_counts, B_counts)\n",
    "\n",
    "            # Output some sequences for debugging.\n",
    "#             self.output(sequences[:10])\n",
    "\n",
    "            # Log likelihood should be increasing\n",
    "            print('iteration %d; log likelihood %.4f' % (iter, log_likelihood))\n",
    "            if previous_llh:\n",
    "                assert log_likelihood >= previous_llh\n",
    "                if log_likelihood - previous_llh < epsilon:\n",
    "                    break\n",
    "            previous_llh = log_likelihood\n",
    "        \n",
    "            iter += 1\n",
    "\n",
    "\n",
    "    def e_step(self, sequences):\n",
    "        # Reset counters of statistics\n",
    "        pi_counts = np.zeros_like(self.pi)\n",
    "        A_counts = np.zeros_like(self.A) \n",
    "        B_counts = np.zeros_like(self.B) \n",
    "        total_log_likelihood = 0.0\n",
    "\n",
    "        for sequence in sequences:\n",
    "            # Run Forward-Backward dynamic program\n",
    "            alpha, beta, gamma, xi, log_likelihood = self.forward_backward(sequence)\n",
    "  \n",
    "            # Accumulate statistics.\n",
    "            pi_counts += gamma[:, 0]\n",
    "            A_counts += xi\n",
    "            for t, x in enumerate(sequence):\n",
    "                B_counts[:, x] += gamma[:, t]\n",
    "            \n",
    "            total_log_likelihood += log_likelihood\n",
    "\n",
    "        return pi_counts, A_counts, B_counts, total_log_likelihood\n",
    "\n",
    "    def m_step(self, pi_counts, A_counts, B_counts):\n",
    "        if not self.fixed_pi:\n",
    "            self.pi = pi_counts / np.sum(pi_counts)\n",
    "        if not self.fixed_A:\n",
    "            self.A = A_counts / np.sum(A_counts, 1, keepdims=True)\n",
    "        if not self.fixed_B:\n",
    "            self.B = B_counts / np.sum(B_counts, 1, keepdims=True)\n",
    "        \n",
    "    def max_posterior_decode(self, sequence):\n",
    "        _, _, gamma, _, log_likelihood = self.forward_backward(sequence)\n",
    "        return np.argmax(gamma, 0)\n",
    "        \n",
    "    def forward_backward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        alpha, log_alpha_norm = self.forward(sequence)\n",
    "        \n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = self.backward(sequence)\n",
    "\n",
    "        # gamma[i][t] = p(z_t = i|x_1, ..., x_T)\n",
    "        gamma = (alpha * beta) / np.sum(alpha * beta, 0)\n",
    "\n",
    "        # xi[i][j] = p(z_t = i, z_{t+1} = j|x_1, ..., x_T)\n",
    "        xi = np.zeros_like(self.A)\n",
    "        for t in range(1, len(sequence)-1):\n",
    "            this_xi = np.zeros_like(self.A)\n",
    "            for i in range(self.num_states):\n",
    "                for j in range(self.num_states):\n",
    "                    this_xi[i, j] += alpha[i, t] * self.A[i, j] * beta[j, t+1] * self.B[j, sequence[t+1]]        \n",
    "            xi += this_xi / np.sum(this_xi)\n",
    "        return alpha, beta, gamma, xi, np.log(np.sum(alpha[:, len(sequence)-1])) + log_alpha_norm[len(sequence)-1]\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        # alpha[i][t] = p(x_1, ..., x_t, z_t = i)\n",
    "        log_alpha_norm = np.zeros(len(sequence))\n",
    "        alpha = np.zeros((len(self.pi), len(sequence)))\n",
    "        alpha[:,0] = self.pi * self.B[:,sequence[0]]\n",
    "        log_alpha_norm[0] = np.log(np.sum(alpha[:,0]))\n",
    "        alpha[:,0] = alpha[:,0] / np.sum(alpha[:,0])\n",
    "        for t in range(1,len(sequence)):\n",
    "            alpha[:,t] = self.B[:,sequence[t]] * self.A.T.dot(alpha[:,t-1])\n",
    "            log_alpha_norm[t] = log_alpha_norm[t - 1] + np.log(np.sum(alpha[:,t]))\n",
    "            alpha[:,t] = alpha[:,t] / np.sum(alpha[:,t])\n",
    "#         assert False, \"Implement forward recursion\"\n",
    "        return alpha, log_alpha_norm\n",
    "    \n",
    "    def backward(self, sequence):\n",
    "        # beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\n",
    "        beta = np.zeros((len(self.pi), len(sequence)))\n",
    "        beta[:,len(sequence) - 1] = np.ones(len(self.pi))\n",
    "        beta[:,len(sequence) - 1] = beta[:,len(sequence) - 1] / np.sum(beta[:,len(sequence) - 1])\n",
    "        for t in range(len(sequence) - 2, -1, -1):\n",
    "            beta[:, t] =  self.A.dot(beta[:, t + 1] * self.B[:,sequence[t + 1]])\n",
    "            beta[:,t] = beta[:,t] / np.sum(beta[:,t])\n",
    "#         assert False, \"Implement backwards recursion to compute betas.\"\n",
    "        return beta\n",
    "\n",
    "    def output(self, sequences):\n",
    "        # Output some decoded states. \n",
    "        for i, sequence in enumerate(sequences):\n",
    "            observations = [self.observations_to_char_mapping[x] for x in sequence]                \n",
    "            map_states = [self.states_to_char_mapping[x] for x in self.max_posterior_decode(sequence)]\n",
    "            print('(states):       %s\\n(observations): %s' % (''.join(map_states), ''.join(observations)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 2**: Implement the assertions in 'forward' and 'backward' methods on the HMM class so that the following block passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -12170.5421\n",
      "iteration 1; log likelihood -10381.2029\n",
      "iteration 2; log likelihood -10345.4999\n",
      "iteration 3; log likelihood -10310.9132\n",
      "iteration 4; log likelihood -10272.9498\n",
      "iteration 5; log likelihood -10230.1673\n",
      "iteration 6; log likelihood -10183.0970\n",
      "iteration 7; log likelihood -10133.8113\n",
      "iteration 8; log likelihood -10084.9661\n",
      "iteration 9; log likelihood -10038.7531\n",
      "iteration 10; log likelihood -9996.3215\n",
      "iteration 11; log likelihood -9957.8073\n",
      "iteration 12; log likelihood -9922.7834\n",
      "iteration 13; log likelihood -9890.7336\n",
      "iteration 14; log likelihood -9861.2057\n",
      "iteration 15; log likelihood -9833.7868\n",
      "iteration 16; log likelihood -9808.1683\n",
      "iteration 17; log likelihood -9784.2283\n",
      "iteration 18; log likelihood -9761.9992\n",
      "iteration 19; log likelihood -9741.5561\n",
      "iteration 20; log likelihood -9722.9258\n",
      "iteration 21; log likelihood -9706.0642\n",
      "iteration 22; log likelihood -9690.8847\n",
      "iteration 23; log likelihood -9677.2941\n",
      "iteration 24; log likelihood -9665.2051\n",
      "iteration 25; log likelihood -9654.5267\n",
      "iteration 26; log likelihood -9645.1454\n",
      "iteration 27; log likelihood -9636.9062\n",
      "iteration 28; log likelihood -9629.6004\n",
      "iteration 29; log likelihood -9622.9643\n",
      "iteration 30; log likelihood -9616.6933\n",
      "iteration 31; log likelihood -9610.4818\n",
      "iteration 32; log likelihood -9604.0960\n",
      "iteration 33; log likelihood -9597.4593\n",
      "iteration 34; log likelihood -9590.6803\n",
      "iteration 35; log likelihood -9583.9765\n",
      "iteration 36; log likelihood -9577.5591\n",
      "iteration 37; log likelihood -9571.5624\n",
      "iteration 38; log likelihood -9566.0376\n",
      "iteration 39; log likelihood -9561.0017\n",
      "iteration 40; log likelihood -9556.4998\n",
      "iteration 41; log likelihood -9552.5737\n",
      "iteration 42; log likelihood -9549.1978\n",
      "iteration 43; log likelihood -9546.3041\n",
      "iteration 44; log likelihood -9543.8242\n",
      "iteration 45; log likelihood -9541.6996\n",
      "iteration 46; log likelihood -9539.8791\n",
      "iteration 47; log likelihood -9538.3165\n",
      "iteration 48; log likelihood -9536.9698\n",
      "iteration 49; log likelihood -9535.8015\n",
      "iteration 50; log likelihood -9534.7788\n",
      "iteration 51; log likelihood -9533.8733\n",
      "iteration 52; log likelihood -9533.0616\n",
      "iteration 53; log likelihood -9532.3241\n",
      "iteration 54; log likelihood -9531.6451\n",
      "iteration 55; log likelihood -9531.0114\n",
      "iteration 56; log likelihood -9530.4123\n",
      "iteration 57; log likelihood -9529.8388\n",
      "iteration 58; log likelihood -9529.2832\n",
      "iteration 59; log likelihood -9528.7392\n",
      "iteration 60; log likelihood -9528.2020\n",
      "iteration 61; log likelihood -9527.6678\n",
      "iteration 62; log likelihood -9527.1343\n",
      "iteration 63; log likelihood -9526.6003\n",
      "iteration 64; log likelihood -9526.0661\n",
      "iteration 65; log likelihood -9525.5336\n",
      "iteration 66; log likelihood -9525.0053\n",
      "iteration 67; log likelihood -9524.4852\n",
      "iteration 68; log likelihood -9523.9772\n",
      "iteration 69; log likelihood -9523.4853\n",
      "iteration 70; log likelihood -9523.0127\n",
      "iteration 71; log likelihood -9522.5616\n",
      "iteration 72; log likelihood -9522.1332\n",
      "iteration 73; log likelihood -9521.7279\n",
      "iteration 74; log likelihood -9521.3453\n",
      "iteration 75; log likelihood -9520.9845\n",
      "iteration 76; log likelihood -9520.6445\n",
      "iteration 77; log likelihood -9520.3241\n",
      "iteration 78; log likelihood -9520.0221\n",
      "iteration 79; log likelihood -9519.7374\n",
      "iteration 80; log likelihood -9519.4687\n",
      "iteration 81; log likelihood -9519.2151\n",
      "iteration 82; log likelihood -9518.9752\n",
      "iteration 83; log likelihood -9518.7482\n",
      "iteration 84; log likelihood -9518.5328\n",
      "iteration 85; log likelihood -9518.3279\n",
      "iteration 86; log likelihood -9518.1323\n",
      "iteration 87; log likelihood -9517.9450\n",
      "iteration 88; log likelihood -9517.7648\n",
      "iteration 89; log likelihood -9517.5907\n",
      "iteration 90; log likelihood -9517.4215\n",
      "iteration 91; log likelihood -9517.2563\n",
      "iteration 92; log likelihood -9517.0941\n",
      "iteration 93; log likelihood -9516.9341\n",
      "iteration 94; log likelihood -9516.7753\n",
      "iteration 95; log likelihood -9516.6169\n",
      "iteration 96; log likelihood -9516.4582\n",
      "iteration 97; log likelihood -9516.2982\n",
      "iteration 98; log likelihood -9516.1360\n",
      "iteration 99; log likelihood -9515.9706\n"
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm.estimate_with_em(encrypted_sequences[:100], parameters={'A' :A,'pi':pi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 3**: Some of the encrypted sequences are quite long. Try decoding some from 'encrypted/99_encrypted.txt' (note these are in Russian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "plaintext_russian = 'plaintext/russian.txt'\n",
    "\n",
    "ciphertext = 'encrypted/99_encrypted.txt' # longer sequences in russian\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping_russian, int_to_char_mapping_russian = get_char_to_int_mapping(plaintext_russian)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext_russian, char_to_int_mapping_russian)\n",
    "encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping_russian)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping_russian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0; log likelihood -75448.5833\n",
      "iteration 1; log likelihood -61235.3633\n",
      "iteration 2; log likelihood -61133.0051\n",
      "iteration 3; log likelihood -60986.8765\n",
      "iteration 4; log likelihood -60752.3970\n",
      "iteration 5; log likelihood -60385.4446\n",
      "iteration 6; log likelihood -59870.7365\n",
      "iteration 7; log likelihood -59233.6394\n",
      "iteration 8; log likelihood -58475.5952\n",
      "iteration 9; log likelihood -57533.5415\n",
      "iteration 10; log likelihood -56398.6531\n",
      "iteration 11; log likelihood -55245.0282\n",
      "iteration 12; log likelihood -54286.1565\n",
      "iteration 13; log likelihood -53581.7584\n",
      "iteration 14; log likelihood -53067.2524\n",
      "iteration 15; log likelihood -52657.1907\n",
      "iteration 16; log likelihood -52296.9903\n",
      "iteration 17; log likelihood -51963.4035\n",
      "iteration 18; log likelihood -51657.1083\n",
      "iteration 19; log likelihood -51383.1091\n",
      "iteration 20; log likelihood -51144.8315\n",
      "iteration 21; log likelihood -50938.5389\n",
      "iteration 22; log likelihood -50758.7364\n",
      "iteration 23; log likelihood -50606.8932\n",
      "iteration 24; log likelihood -50484.6140\n",
      "iteration 25; log likelihood -50389.2584\n",
      "iteration 26; log likelihood -50315.8621\n",
      "iteration 27; log likelihood -50259.3710\n",
      "iteration 28; log likelihood -50215.5610\n",
      "iteration 29; log likelihood -50181.1947\n",
      "iteration 30; log likelihood -50153.9405\n",
      "iteration 31; log likelihood -50132.1667\n",
      "iteration 32; log likelihood -50114.6805\n",
      "iteration 33; log likelihood -50100.5587\n",
      "iteration 34; log likelihood -50089.0744\n",
      "iteration 35; log likelihood -50079.6462\n",
      "iteration 36; log likelihood -50071.8064\n",
      "iteration 37; log likelihood -50065.1894\n",
      "iteration 38; log likelihood -50059.5208\n",
      "iteration 39; log likelihood -50054.6016\n",
      "iteration 40; log likelihood -50050.2911\n",
      "iteration 41; log likelihood -50046.4915\n",
      "iteration 42; log likelihood -50043.1347\n",
      "iteration 43; log likelihood -50040.1707\n",
      "iteration 44; log likelihood -50037.5598\n",
      "iteration 45; log likelihood -50035.2666\n",
      "iteration 46; log likelihood -50033.2572\n",
      "iteration 47; log likelihood -50031.4981\n",
      "iteration 48; log likelihood -50029.9564\n",
      "iteration 49; log likelihood -50028.6007\n",
      "iteration 50; log likelihood -50027.4024\n",
      "iteration 51; log likelihood -50026.3378\n",
      "iteration 52; log likelihood -50025.3889\n",
      "iteration 53; log likelihood -50024.5443\n",
      "iteration 54; log likelihood -50023.7963\n",
      "iteration 55; log likelihood -50023.1371\n",
      "iteration 56; log likelihood -50022.5564\n",
      "iteration 57; log likelihood -50022.0426\n",
      "iteration 58; log likelihood -50021.5845\n",
      "iteration 59; log likelihood -50021.1731\n",
      "iteration 60; log likelihood -50020.8010\n",
      "iteration 61; log likelihood -50020.4628\n",
      "iteration 62; log likelihood -50020.1540\n",
      "iteration 63; log likelihood -50019.8712\n",
      "iteration 64; log likelihood -50019.6116\n",
      "iteration 65; log likelihood -50019.3729\n",
      "iteration 66; log likelihood -50019.1531\n",
      "iteration 67; log likelihood -50018.9501\n",
      "iteration 68; log likelihood -50018.7621\n",
      "iteration 69; log likelihood -50018.5873\n",
      "iteration 70; log likelihood -50018.4239\n",
      "iteration 71; log likelihood -50018.2703\n",
      "iteration 72; log likelihood -50018.1252\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-5d43c65f6920>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Estimate the parameters and decode the encrypted sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhmm_russian\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_with_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36mestimate_with_em\u001b[0;34m(self, sequences, parameters, epsilon, max_iters)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Infer expected counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mpi_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;31m# Update parameters based on counts.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36me_step\u001b[0;34m(self, sequences)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# Run Forward-Backward dynamic program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# Accumulate statistics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                     \u001b[0mthis_xi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mxi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mthis_xi\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis_xi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_alpha_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Since it's a substitution cipher we assume hidden states and observations have same alphabet.\n",
    "state_to_char_mapping_russian = int_to_char_mapping_russian\n",
    "observation_to_char_mapping_russian = int_to_char_mapping_russian\n",
    "\n",
    "# Initialize a HMM with the correct state/output spaces.\n",
    "hmm_russian = HMM(observation_to_char_mapping_russian, state_to_char_mapping_russian)\n",
    "\n",
    "# Estimate the parameters and decode the encrypted sequences.\n",
    "hmm_russian.estimate_with_em(encrypted_sequences[:100], parameters={'A' :A,'pi':pi}, max_iters = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 4**: Make your implementation of forward and backward more efficient by removing all but the outermost for-loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 5**: Try to classify the author of each text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data to use.\n",
    "# plaintext = 'plaintext/shakespeare.txt'\n",
    "plaintext = 'plaintext/all.txt'\n",
    "\n",
    "\n",
    "# load a character to integer mapping and reverse                                                                                                         \n",
    "char_to_int_mapping, int_to_char_mapping = get_char_to_int_mapping(plaintext)\n",
    "\n",
    "# load sequences as ints                                                                                                                                  \n",
    "plaintext_sequences = load_sequences(plaintext, char_to_int_mapping)\n",
    "\n",
    "# estimate a markov model over characters                                                                                                                 \n",
    "pi, A = estimate_markov_model_from_sequences(plaintext_sequences, len(char_to_int_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encrypted/0_encrypted.txt\n",
      "iteration 0; log likelihood -19277.3554\n",
      "iteration 1; log likelihood -12696.4513\n",
      "iteration 2; log likelihood -12659.3748\n",
      "iteration 3; log likelihood -12621.0184\n",
      "iteration 4; log likelihood -12574.8023\n",
      "iteration 5; log likelihood -12518.2091\n",
      "iteration 6; log likelihood -12448.1019\n",
      "iteration 7; log likelihood -12364.1777\n",
      "iteration 8; log likelihood -12273.4970\n",
      "iteration 9; log likelihood -12186.4379\n",
      "iteration 10; log likelihood -12108.7312\n",
      "iteration 11; log likelihood -12040.5197\n",
      "iteration 12; log likelihood -11979.9522\n",
      "iteration 13; log likelihood -11925.4837\n",
      "iteration 14; log likelihood -11876.4580\n",
      "iteration 15; log likelihood -11832.9292\n",
      "iteration 16; log likelihood -11795.1249\n",
      "iteration 17; log likelihood -11762.8533\n",
      "iteration 18; log likelihood -11735.3892\n",
      "iteration 19; log likelihood -11711.8632\n",
      "iteration 20; log likelihood -11691.5979\n",
      "iteration 21; log likelihood -11674.1026\n",
      "iteration 22; log likelihood -11658.8772\n",
      "iteration 23; log likelihood -11645.4139\n",
      "iteration 24; log likelihood -11633.3326\n",
      "iteration 25; log likelihood -11622.3704\n",
      "iteration 26; log likelihood -11612.3245\n",
      "iteration 27; log likelihood -11603.0303\n",
      "iteration 28; log likelihood -11594.3452\n",
      "iteration 29; log likelihood -11586.1493\n",
      "iteration 30; log likelihood -11578.3625\n",
      "iteration 31; log likelihood -11570.9828\n",
      "iteration 32; log likelihood -11564.1179\n",
      "iteration 33; log likelihood -11557.9004\n",
      "iteration 34; log likelihood -11552.3811\n",
      "iteration 35; log likelihood -11547.5406\n",
      "iteration 36; log likelihood -11543.3271\n",
      "iteration 37; log likelihood -11539.6715\n",
      "iteration 38; log likelihood -11536.4947\n",
      "iteration 39; log likelihood -11533.7168\n",
      "iteration 40; log likelihood -11531.2644\n",
      "iteration 41; log likelihood -11529.0799\n",
      "iteration 42; log likelihood -11527.1274\n",
      "iteration 43; log likelihood -11525.3869\n",
      "iteration 44; log likelihood -11523.8333\n",
      "iteration 45; log likelihood -11522.4289\n",
      "iteration 46; log likelihood -11521.1348\n",
      "iteration 47; log likelihood -11519.9175\n",
      "iteration 48; log likelihood -11518.7487\n",
      "iteration 49; log likelihood -11517.6048\n",
      "iteration 50; log likelihood -11516.4732\n",
      "iteration 51; log likelihood -11515.3655\n",
      "iteration 52; log likelihood -11514.3198\n",
      "iteration 53; log likelihood -11513.3695\n",
      "iteration 54; log likelihood -11512.5150\n",
      "iteration 55; log likelihood -11511.7379\n",
      "iteration 56; log likelihood -11511.0220\n",
      "iteration 57; log likelihood -11510.3563\n",
      "iteration 58; log likelihood -11509.7338\n",
      "iteration 59; log likelihood -11509.1504\n",
      "iteration 60; log likelihood -11508.6044\n",
      "iteration 61; log likelihood -11508.0957\n",
      "iteration 62; log likelihood -11507.6244\n",
      "iteration 63; log likelihood -11507.1890\n",
      "iteration 64; log likelihood -11506.7867\n",
      "iteration 65; log likelihood -11506.4129\n",
      "iteration 66; log likelihood -11506.0624\n",
      "iteration 67; log likelihood -11505.7297\n",
      "iteration 68; log likelihood -11505.4094\n",
      "iteration 69; log likelihood -11505.0962\n",
      "iteration 70; log likelihood -11504.7848\n",
      "iteration 71; log likelihood -11504.4707\n",
      "iteration 72; log likelihood -11504.1502\n",
      "iteration 73; log likelihood -11503.8215\n",
      "iteration 74; log likelihood -11503.4855\n",
      "iteration 75; log likelihood -11503.1448\n",
      "iteration 76; log likelihood -11502.8025\n",
      "iteration 77; log likelihood -11502.4603\n",
      "iteration 78; log likelihood -11502.1180\n",
      "iteration 79; log likelihood -11501.7729\n",
      "iteration 80; log likelihood -11501.4209\n",
      "iteration 81; log likelihood -11501.0567\n",
      "iteration 82; log likelihood -11500.6749\n",
      "iteration 83; log likelihood -11500.2718\n",
      "iteration 84; log likelihood -11499.8496\n",
      "iteration 85; log likelihood -11499.4204\n",
      "iteration 86; log likelihood -11499.0037\n",
      "iteration 87; log likelihood -11498.6147\n",
      "iteration 88; log likelihood -11498.2578\n",
      "iteration 89; log likelihood -11497.9299\n",
      "iteration 90; log likelihood -11497.6262\n",
      "iteration 91; log likelihood -11497.3427\n",
      "iteration 92; log likelihood -11497.0766\n",
      "iteration 93; log likelihood -11496.8257\n",
      "iteration 94; log likelihood -11496.5882\n",
      "iteration 95; log likelihood -11496.3627\n",
      "iteration 96; log likelihood -11496.1480\n",
      "iteration 97; log likelihood -11495.9428\n",
      "iteration 98; log likelihood -11495.7458\n",
      "iteration 99; log likelihood -11495.5553\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (69,) (69,61) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-beda16b92e62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mall_encrypted_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mciphertext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencrypted_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_with_em\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'A'\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pi'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mall_decrypted_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mciphertext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_posterior_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencrypted_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36mmax_posterior_decode\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmax_posterior_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36mforward_backward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# alpha[i][t] = p(x_1, ..., x_t, z_t = i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_alpha_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;31m# beta[i][t] = p(x_t+1, ..., x_T|z_t = i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-428f351344f1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sequence)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mlog_alpha_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0mlog_alpha_norm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (69,) (69,61) "
     ]
    }
   ],
   "source": [
    "all_encrypted_sequences = {}\n",
    "all_decrypted_sequences = {}\n",
    "\n",
    "state_to_char_mapping = int_to_char_mapping\n",
    "observation_to_char_mapping = int_to_char_mapping\n",
    "hmm = HMM(observation_to_char_mapping, state_to_char_mapping)\n",
    "\n",
    "for i in range(143):\n",
    "    ciphertext = 'encrypted/' + str(i) + '_encrypted.txt' # longer sequences in russian\n",
    "    print(ciphertext)\n",
    "    encrypted_sequences = load_sequences(ciphertext, char_to_int_mapping)\n",
    "    all_encrypted_sequences[ciphertext] = encrypted_sequences\n",
    "    hmm.estimate_with_em(encrypted_sequences[:100], parameters={'A' :A,'pi':pi}, max_iters = 100)\n",
    "    all_decrypted_sequences[ciphertext] = hmm.max_posterior_decode(encrypted_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,gamma,_ = hmm_russian.forward_backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
