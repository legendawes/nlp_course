{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Copy of seminar.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YgaZQ8ue4BH",
        "colab_type": "text"
      },
      "source": [
        "### N-gram language models or how to write scientific papers (4 pts)\n",
        "\n",
        "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
        "\n",
        "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
        "\n",
        "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
        "\n",
        "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zya3fPWce4BK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSihApwCe4BN",
        "colab_type": "code",
        "outputId": "4fbdee24-a414-4cc4-c97e-f2c4d41bec0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
        "!wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
        "!tar -xvzf arxivData.json.tar.gz\n",
        "data = pd.read_json(\"./arxivData.json\")\n",
        "data.sample(n=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-08 15:46:16--  https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.1, 2620:100:6021:1::a27d:4101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz [following]\n",
            "--2019-10-08 15:46:16--  https://www.dropbox.com/s/dl/99az9n1b57qkd9j/arxivData.json.tar.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com/cd/0/get/AqBKJF1GHsC_pU4BjXDdSmnHojTMUsanUpJv6xfW-OcONNjfM54AhJR9U2wzT2qPdmIGesrmMVThCeeLRboTfUDKc1CLTVOvb5Lr1N1vTchavg/file?dl=1# [following]\n",
            "--2019-10-08 15:46:17--  https://uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com/cd/0/get/AqBKJF1GHsC_pU4BjXDdSmnHojTMUsanUpJv6xfW-OcONNjfM54AhJR9U2wzT2qPdmIGesrmMVThCeeLRboTfUDKc1CLTVOvb5Lr1N1vTchavg/file?dl=1\n",
            "Resolving uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com (uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com)... 162.125.65.6, 2620:100:6021:6::a27d:4106\n",
            "Connecting to uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com (uc3453a73117b9a43709feb766a8.dl.dropboxusercontent.com)|162.125.65.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 18933283 (18M) [application/binary]\n",
            "Saving to: ‘arxivData.json.tar.gz’\n",
            "\n",
            "arxivData.json.tar. 100%[===================>]  18.06M  54.6MB/s    in 0.3s    \n",
            "\n",
            "2019-10-08 15:46:18 (54.6 MB/s) - ‘arxivData.json.tar.gz’ saved [18933283/18933283]\n",
            "\n",
            "arxivData.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>day</th>\n",
              "      <th>id</th>\n",
              "      <th>link</th>\n",
              "      <th>month</th>\n",
              "      <th>summary</th>\n",
              "      <th>tag</th>\n",
              "      <th>title</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18897</th>\n",
              "      <td>[{'name': 'Dai Xu'}, {'name': 'Xiaowang Zhang'...</td>\n",
              "      <td>10</td>\n",
              "      <td>1301.2137v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>1</td>\n",
              "      <td>This paper presents a novel approach based on ...</td>\n",
              "      <td>[{'term': 'cs.AI', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Forgetting-based Approach to Merging Knowled...</td>\n",
              "      <td>2013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12217</th>\n",
              "      <td>[{'name': 'Matthew Ragoza'}, {'name': 'Lillian...</td>\n",
              "      <td>20</td>\n",
              "      <td>1710.07400v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>Docking is an important tool in computational ...</td>\n",
              "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
              "      <td>Ligand Pose Optimization with Atomic Grid-Base...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35641</th>\n",
              "      <td>[{'name': 'Pedro Cabalar'}]</td>\n",
              "      <td>22</td>\n",
              "      <td>1011.4833v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>11</td>\n",
              "      <td>In this paper we consider a logical treatment ...</td>\n",
              "      <td>[{'term': 'cs.LO', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A Logical Charaterisation of Ordered Disjunction</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17372</th>\n",
              "      <td>[{'name': 'Andres Campero'}, {'name': 'Bjarke ...</td>\n",
              "      <td>23</td>\n",
              "      <td>1710.08048v1</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>10</td>\n",
              "      <td>We explore the representational space of emoti...</td>\n",
              "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>A First Step in Combining Cognitive Event Feat...</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24889</th>\n",
              "      <td>[{'name': 'Ross Goroshin'}, {'name': 'Joan Bru...</td>\n",
              "      <td>18</td>\n",
              "      <td>1412.6056v6</td>\n",
              "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
              "      <td>12</td>\n",
              "      <td>Current state-of-the-art classification and de...</td>\n",
              "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
              "      <td>Unsupervised Learning of Spatiotemporally Cohe...</td>\n",
              "      <td>2014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  author  ...  year\n",
              "18897  [{'name': 'Dai Xu'}, {'name': 'Xiaowang Zhang'...  ...  2013\n",
              "12217  [{'name': 'Matthew Ragoza'}, {'name': 'Lillian...  ...  2017\n",
              "35641                        [{'name': 'Pedro Cabalar'}]  ...  2010\n",
              "17372  [{'name': 'Andres Campero'}, {'name': 'Bjarke ...  ...  2017\n",
              "24889  [{'name': 'Ross Goroshin'}, {'name': 'Joan Bru...  ...  2014\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svRvTVOpe4BP",
        "colab_type": "code",
        "outputId": "8bdec717-171e-4156-ccba-1883ad64cb1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# assemble lines: concatenate title and description\n",
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'], axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
              " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
              " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t25XtvFee4BR",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization\n",
        "\n",
        "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU3-Rny8e4BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Task: convert lines (in-place) into strings of space-separated tokens. import & use WordPunctTokenizer\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "# <YOUR CODE>\n",
        "lines = [' '.join(tokenizer.tokenize(line.lower())) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yRvgkuge4BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert sorted(lines, key=len)[0] == \\\n",
        "    'differential contrastive divergence ; this paper has been retracted .'\n",
        "assert sorted(lines, key=len)[2] == \\\n",
        "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntQp8J_Ne4BX",
        "colab_type": "text"
      },
      "source": [
        "### N-Gram Language Model\n",
        "\n",
        "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
        "\n",
        "It can do so by following the chain rule:\n",
        "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
        "\n",
        "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
        "\n",
        "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
        "\n",
        "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
        "\n",
        "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
        "\n",
        "$$\n",
        "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
        "$$\n",
        "\n",
        "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBt7z30Pe4BY",
        "colab_type": "text"
      },
      "source": [
        "The first stage to building such a model is counting all word occurences given N-1 previous words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HuNTI9ae4BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "from collections import defaultdict, Counter, deque\n",
        "\n",
        "# special tokens: \n",
        "# - unk represents absent tokens, \n",
        "# - eos is a special token after the end of sequence\n",
        "\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "\n",
        "def count_ngrams(lines, n):\n",
        "    \"\"\"\n",
        "    Count how many times each word occured after (n - 1) previous words\n",
        "    :param lines: an iterable of strings with space-separated tokens\n",
        "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
        "\n",
        "    When building counts, please consider the following two edge cases\n",
        "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
        "      empty prefix: \"\" -> (UNK, UNK)\n",
        "      short prefix: \"the\" -> (UNK, the)\n",
        "      long prefix: \"the new approach\" -> (new, approach)\n",
        "    - you should add a special token, EOS, at the end of each sequence\n",
        "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
        "      count the probability of this token just like all others.\n",
        "    \"\"\"\n",
        "    counts = defaultdict(Counter)\n",
        "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
        "    \n",
        "    for line in lines:\n",
        "      prev_words = deque([UNK] * (n-1), maxlen=(n-1))\n",
        "      for word in line.split() + [EOS]:\n",
        "        counts[tuple(prev_words)][word] += 1\n",
        "        prev_words.append(word)\n",
        "\n",
        "    \n",
        "    return counts\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zWkjOv6e4Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's test it\n",
        "dummy_lines = sorted(lines, key=len)[:100]\n",
        "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
        "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
        "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
        "assert dummy_counts['p', '=']['np'] == 2\n",
        "assert dummy_counts['author', '.']['_EOS_'] == 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSW_q0O9e4Bc",
        "colab_type": "text"
      },
      "source": [
        "Once we can count N-grams, we can build a probabilistic language model.\n",
        "The simplest way to compute probabilities is in proporiton to counts:\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEvRi_moe4Bd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLanguageModel:    \n",
        "    def __init__(self, lines, n):\n",
        "        \"\"\" \n",
        "        Train a simple count-based language model: \n",
        "        compute probabilities P(w_t | prefix) given ngram counts\n",
        "        \n",
        "        :param n: computes probability of next token given (n - 1) previous words\n",
        "        :param lines: an iterable of strings with space-separated tokens\n",
        "        \"\"\"\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "    \n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        \n",
        "        # compute token proabilities given counts\n",
        "        self.probs = defaultdict(dict)\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "        \n",
        "        for prev_words, next_word_counts in counts.items():\n",
        "          total_counts = sum(next_word_counts.values())\n",
        "\n",
        "          for next_word, next_word_count in next_word_counts.items():\n",
        "            self.probs[prev_words][next_word] = next_word_count / total_counts\n",
        "        # populate self.probs with actual probabilities\n",
        "#         <YOUR CODE>\n",
        "            \n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
        "        \"\"\"\n",
        "        prefix = prefix.split()\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "        return self.probs[tuple(prefix)]\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        \"\"\"\n",
        "        :param prefix: string with space-separated prefix tokens\n",
        "        :param next_token: the next token to predict probability for\n",
        "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
        "        \"\"\"\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGbt7NfPe4Bf",
        "colab_type": "text"
      },
      "source": [
        "Let's test it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4hGfb1Me4Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
        "\n",
        "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
        "assert np.allclose(p_initial['learning'], 0.02)\n",
        "assert np.allclose(p_initial['a'], 0.13)\n",
        "assert np.allclose(p_initial.get('meow', 0), 0)\n",
        "assert np.allclose(sum(p_initial.values()), 1)\n",
        "\n",
        "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
        "assert np.allclose(p_a['machine'], 0.15384615)\n",
        "assert np.allclose(p_a['note'], 0.23076923)\n",
        "assert np.allclose(p_a.get('the', 0), 0)\n",
        "assert np.allclose(sum(p_a.values()), 1)\n",
        "\n",
        "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
        "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
        "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
        "    \"your 3-gram model should only depend on 2 previous words\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-QJq55ne4Bk",
        "colab_type": "text"
      },
      "source": [
        "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyfNoktse4Bl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hb0nOLe4Bo",
        "colab_type": "text"
      },
      "source": [
        "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "__forever:__\n",
        "* $w_{next} \\sim P(w_{next} | X)$\n",
        "* $X = concat(X, w_{next})$\n",
        "\n",
        "\n",
        "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adY8tZmJe4Bo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "    \"\"\"\n",
        "    return next token after prefix;\n",
        "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
        "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
        "    \"\"\"\n",
        "    \n",
        "    word_prob_dict = lm.get_possible_next_tokens(prefix)\n",
        "    words, probs = zip(*word_prob_dict.items())\n",
        "    probs = np.array(probs)\n",
        "    \n",
        "    if not (np.allclose(temperature, 0.0)):\n",
        "      new_probs = probs ** (1 / temperature)\n",
        "      new_probs /= new_probs.sum()\n",
        "      next_token = np.random.choice(words, p=new_probs)\n",
        "    else:\n",
        "      next_token = words[np.argmax(probs)]\n",
        "    \n",
        "    return next_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hzsIHFse4Br",
        "colab_type": "code",
        "outputId": "cfbb3947-ac9b-4017-d3b2-e78029e91faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200\n",
        "\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs['learning'] < 3000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs['learning'] < 9000\n",
        "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs['learning'] == 10000\n",
        "\n",
        "print(\"Looks nice!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looks nice!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnGM5mqIe4Bt",
        "colab_type": "text"
      },
      "source": [
        "Let's have fun with this model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZdUZCUqe4Bu",
        "colab_type": "code",
        "outputId": "64ce2822-c16e-4711-8ed4-46069f2b8c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "prefix = 'bias' # <- your ideas :)\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias and presentation service for the motion of background knowledge , this paper , to compute the exact recovery } of the fcnn and the visible distribution of a certain convex function of the models on two classical discrete black - box neural architectures . _EOS_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWkCFs0le4Bw",
        "colab_type": "code",
        "outputId": "cc74f284-b8f3-4f27-e342-6ccaf4519644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "prefix = 'god' # <- more of your ideas\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "god ( s ) ( e . g ., cnns and a set of samples . the key idea is to develop a new method for estimating the gradient of the proposed method is used to model the appearance and motion dynamics . we then propose a method for the purpose of this algorithm in terms of the art on the other hand , the proposed method is to find the best of our approach achieves state - of - the - art methods . _EOS_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61FBoe6le4Bx",
        "colab_type": "text"
      },
      "source": [
        "### Evaluating language models: perplexity\n",
        "\n",
        "Perplexity is a measure of how well does your model approximate true probability distribution behind data. __Smaller perplexity = better model__.\n",
        "\n",
        "To compute perplexity on one sentence, use:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n",
        "\n",
        "\n",
        "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of 1, divided by __total length of all sentences__ in corpora.\n",
        "\n",
        "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4MUyvmpsVXi",
        "colab_type": "code",
        "outputId": "984b9794-9e8b-4705-ecbf-e6018798b178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "# for line in lines:\n",
        "n = lm.n\n",
        "line = 'god ( s ) and the methods of computer vision and robotics .'\n",
        "prev_words = deque(maxlen=(n-1))\n",
        "for word in line.split(' ') + [EOS]:\n",
        "  prefix = ' '.join(prev_words)\n",
        "  prob = lm.get_next_token_prob(prefix, word)\n",
        "  print(prefix, word, prob)\n",
        "#   counts[tuple(prev_words)][word] += 1\n",
        "#   prev_words.append(word)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " god 2.4390243902439026e-05\n",
            " ( 0.00024390243902439024\n",
            " s 0.00014634146341463414\n",
            " ) 0\n",
            " and 4.878048780487805e-05\n",
            " the 0.022268292682926828\n",
            " methods 0.00021951219512195122\n",
            " of 4.878048780487805e-05\n",
            " computer 0.0004878048780487805\n",
            " vision 0.0005121951219512195\n",
            " and 4.878048780487805e-05\n",
            " robotics 0\n",
            " . 0\n",
            " _EOS_ 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaWPp0wfe4By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    \"\"\"\n",
        "    :param lines: a list of strings with space-separated tokens\n",
        "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
        "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
        "    \n",
        "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
        "    \n",
        "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
        "    \"\"\"\n",
        "    N_tokens = 0\n",
        "    n_words_in_context = lm.n - 1\n",
        "    log_perplex = 0\n",
        "    \n",
        "    for line in lines:\n",
        "      prev_words = deque(maxlen=n_words_in_context)\n",
        "      for word in line.split() + [EOS]:\n",
        "        prefix = ' '.join(prev_words)\n",
        "        log_prob = np.log(lm.get_next_token_prob(prefix, word))\n",
        "        \n",
        "        log_perplex += max(log_prob, min_logprob)\n",
        "        N_tokens += 1\n",
        "        prev_words.append(word)\n",
        "        \n",
        "    perp = np.exp(- log_perplex / N_tokens)\n",
        "    \n",
        "    return perp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PryY8gnYe4B1",
        "colab_type": "code",
        "outputId": "ca67c29c-e964-43e4-ec2e-79694243cfc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
        "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
        "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, dummy_lines)\n",
        "ppx3 = perplexity(lm3, dummy_lines)\n",
        "ppx10 = perplexity(lm10, dummy_lines)\n",
        "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
        "\n",
        "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be nonnegative and reasonably small\"\n",
        "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
        "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
        "    \" Make sure you use min_logprob right\"\n",
        "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v13dNrN_e4B4",
        "colab_type": "text"
      },
      "source": [
        "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6ff4Nkre4B4",
        "colab_type": "code",
        "outputId": "332ded00-298c-4592-d15f-bd01658dbc60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 1832.23136\n",
            "N = 2, Perplexity = 85653987.28774\n",
            "N = 3, Perplexity = 61999196259043346743296.00000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsB12CV4e4B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# whoops, it just blew up :)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyHRfudue4B_",
        "colab_type": "text"
      },
      "source": [
        "### LM Smoothing\n",
        "\n",
        "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
        "\n",
        "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
        "\n",
        "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
        "\n",
        "Here's an example code we've implemented for you:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeeLX4Pqe4CA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel): \n",
        "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTIdud3Be4CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
        "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7pHGCF8e4CF",
        "colab_type": "code",
        "outputId": "17e92169-cd8b-48f2-d958-814c7b8048b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "N = 1, Perplexity = 977.67559\n",
            "N = 2, Perplexity = 470.48021\n",
            "N = 3, Perplexity = 3679.44765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxrvFTtre4CH",
        "colab_type": "code",
        "outputId": "3ac85a14-982d-4d44-c7ea-3b33709acd52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# optional: try to sample tokens from such a model\n",
        "# i'm on it!\n",
        "lm = LaplaceLanguageModel(train_lines, n=2, delta=0.1)\n",
        "prefix = 'bias' # <- your ideas :)\n",
        "\n",
        "for i in range(10):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "        \n",
        "print(prefix)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bias xslit synaptically rangle sunda verbally hackers nascent mcclelland downsample hardly\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmJekkBde4CJ",
        "colab_type": "text"
      },
      "source": [
        "### Kneser-Ney smoothing\n",
        "\n",
        "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
        "\n",
        "\n",
        "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
        "\n",
        "It can be computed recurrently, for n>1:\n",
        "\n",
        "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
        "\n",
        "where\n",
        "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
        "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
        "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
        "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
        "\n",
        "See lecture slides or wiki for more detailed formulae.\n",
        "\n",
        "__Your task__ is to\n",
        "- implement KneserNeyLanguageModel\n",
        "- test it on 1-3 gram language models\n",
        "- find optimal (within one order of magnitude) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFtG4q3re4CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KneserNeyLanguageModel(NGramLanguageModel): \n",
        "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
        "    def __init__(self, lines, n, delta=5.0):\n",
        "        self.n = n\n",
        "        self.delta = delta\n",
        "        \n",
        "        self.all_ngram_counts = defaultdict(Counter)\n",
        "        for n_current in range(1, self.n+1):\n",
        "          ngrams = count_ngrams(lines, n_current)\n",
        "          self.all_ngram_counts.update(ngrams)\n",
        "        self.all_combs_of_len = defaultdict(set)\n",
        "        self.all_combs_of_len_count = defaultdict(int)\n",
        "\n",
        "        for prefix, counts in dummy_lm.all_ngram_counts.items():\n",
        "          for word in counts:\n",
        "            self.all_combs_of_len[len(prefix) + 1].add(prefix + (word,))\n",
        "            \n",
        "        for phrase_len, tokens_set in all_combs_of_len.items():\n",
        "          self.all_combs_of_len_count[phrase_len] = len(tokens_set)\n",
        "        \n",
        "        self.pkn = self._init_pkn(lines)\n",
        "        self.probs = self._init_all_models(lines)\n",
        "        self.vocab = set(token for token_counts in self.all_ngram_counts.values() for token in token_counts)\n",
        "        \n",
        "        \n",
        "        \n",
        "    def _init_pkn(self, lines):\n",
        "      word_prev_words_set = defaultdict(set)\n",
        "      prev_word = UNK\n",
        "\n",
        "      for line in lines:\n",
        "        for word in line.split() + [EOS]:\n",
        "          word_prev_words_set[word].add(prev_word)\n",
        "          prev_word = word\n",
        "\n",
        "        word_prev_words_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "        normalization = sum([len(x) for x in word_prev_words_set.values()])\n",
        "\n",
        "      for word, prev_words_set in word_prev_words_set.items():\n",
        "        word_prev_words_prob[()][word] = len(prev_words_set) / normalization\n",
        "\n",
        "      return word_prev_words_prob\n",
        "    \n",
        "    \n",
        "    def _init_all_models(self, lines):\n",
        "      prev_model = self.pkn\n",
        "      models_dict = {0: prev_model.copy()}\n",
        "      all_lens_dict = prev_model.copy()\n",
        "\n",
        "      for context_len in range(1, self.n):\n",
        "        new_model = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "        for line in lines:\n",
        "          prefix = deque([UNK] * context_len, maxlen=context_len)\n",
        "          for word in line.split() + [EOS]:\n",
        "            k_n = self.count_kneser_ney(tuple(prefix), word, delta=self.delta, previous_model=prev_model)\n",
        "            new_model[tuple(prefix)][word] = k_n\n",
        "            prefix.append(word)\n",
        "\n",
        "        models_dict[context_len] = new_model.copy()\n",
        "        prev_model = new_model.copy()\n",
        "        \n",
        "        all_lens_dict.update(new_model)\n",
        "      \n",
        "      return all_lens_dict\n",
        "    \n",
        "    \n",
        "    def count_prefix_n_one_plus(self, prefix):\n",
        "      return len(self.all_ngram_counts[prefix])  \n",
        "\n",
        "    def count_first_term(self, prefix, word, delta):\n",
        "      if isinstance(word, str):\n",
        "        word = (word, )\n",
        "\n",
        "      phrase = prefix + word\n",
        "      phrase_count = sum(self.all_ngram_counts[phrase].values())\n",
        "\n",
        "      phrase_count = max(phrase_count - delta, 0)\n",
        "      \n",
        "      normalization = self.all_combs_of_len_count[len(prefix)]\n",
        "      #sum(self.all_ngram_counts[prefix].values()) #prefix_len_count[len(phrase)]\n",
        "      \n",
        "      try:\n",
        "        return phrase_count / normalization\n",
        "      except ZeroDivisionError as e:\n",
        "        print(self.all_combs_of_len_count[len(phrase)])\n",
        "        print(prefix, len(prefix))\n",
        "        raise e\n",
        "      \n",
        "\n",
        "    def count_second_term(self, prefix, word, delta, previous_model):\n",
        "      if isinstance(word, str):\n",
        "        word = (word, )\n",
        "\n",
        "      phrase = prefix + word\n",
        "      normalization = self.all_combs_of_len_count[len(phrase)]\n",
        "#       normalization = sum(self.all_ngram_counts[prefix].values())\n",
        "      n_one_plus = self.count_prefix_n_one_plus(prefix)\n",
        "\n",
        "      shorter_prefix = prefix[1:]\n",
        "      pkn = previous_model[shorter_prefix][word[0]]\n",
        "\n",
        "      return delta * n_one_plus * pkn / normalization\n",
        "\n",
        "\n",
        "    def count_kneser_ney(self, prefix, word, delta, previous_model):\n",
        "      first_term = self.count_first_term(prefix, word, delta)\n",
        "      second_term = self.count_second_term(prefix, word, delta, previous_model)\n",
        "      return first_term + second_term\n",
        "    \n",
        "    \n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "      return super().get_possible_next_tokens(prefix)\n",
        "    \n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "      token_probs = super().get_possible_next_tokens(prefix)\n",
        "      if next_token in token_probs:\n",
        "          return token_probs[next_token]\n",
        "      else:\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNmVTpv9Jx6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_lm = KneserNeyLanguageModel(very_dummy_lines, n=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmwbaCPWe4CM",
        "colab_type": "code",
        "outputId": "ae1ad73a-946a-4e23-c694-34ed846ff0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#test that it's a valid probability model\n",
        "for n in (1, 2, 3):\n",
        "    dummy_lm = KneserNeyLanguageModel(very_dummy_lines, n=n)\n",
        "    \n",
        "    sum_of_probs = sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab])\n",
        "    print(n, sum_of_probs)\n",
        "    \n",
        "    if not np.allclose(sum_of_probs, 1):\n",
        "      print('not ok')\n",
        "#     assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 1.0\n",
            "2 0.16666666666666666\n",
            "not ok\n",
            "3 0\n",
            "not ok\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSOJKmIne4CN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = KneserNeyLanguageModel(train_lines, n=n, smoothing=<...>)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKvfPaodBHwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
